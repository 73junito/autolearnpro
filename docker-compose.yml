version: '3.8'

services:
  db:
    image: postgres:15
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: lms_api_dev
    volumes:
      - db_data:/var/lib/postgresql/data

    ports:
      - "5432:5432"

  backend:
    build: ./backend/lms_api
    dns:
      - 8.8.8.8
      - 1.1.1.1
    command: sh -c "mix local.rebar --force && mix local.hex --force && mix deps.get && mix ecto.setup || true; MIX_ENV=dev mix phx.server"
    environment:
      DATABASE_URL: "ecto://postgres:postgres@db:5432/lms_api_dev"
      MIX_ENV: dev
      SECRET_KEY_BASE: "dev_secret"
      PORT: 4000
    ports:
      - "4000:4000"
    depends_on:
      - db

  frontend:
    build: ./frontend/web
    environment:
      NEXT_PUBLIC_API_URL: "http://localhost:4000/api"
    ports:
      - "3000:3000"
    depends_on:
      - backend

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    # Use the image entrypoint; run model pulls manually or via an init container/job
    ports:
      - "11435:11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_LOAD_TIMEOUT=300s
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_DEBUG=INFO
    volumes:
      - ./ollama_models:/root/.ollama/models
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep -q '/bin/ollama serve' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    # Grant access to the host GPU when supported by the Docker Engine
    gpus: all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # GPU monitoring stack
  # Note: DCGM exporter is not supported reliably under WSL2/Docker Desktop
  # because DCGM expects PCI sysfs entries like local_cpulist which are
  # not exposed. We remove dcgm-exporter and use the nvidia-smi exporter instead.

  prometheus:
    image: prom/prometheus:latest
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    # For Compose implementations that support device_requests, uncomment and use instead:
    # device_requests:
    #   - driver: "nvidia"
    #     count: -1
    #     capabilities: ["gpu"]

  nvidia-smi-exporter:
    build:
      context: ./monitoring/nvidia_smi_exporter
    restart: unless-stopped
    ports:
      - "9401:9401"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    gpus: all
volumes:
  db_data:

  # Lightweight nvidia-smi exporter for WSL/Docker Desktop environments
  # (Used as an alternative to dcgm-exporter when DCGM cannot access PCI sysfs)
  
